import os
import warnings
warnings.filterwarnings("ignore")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score, KFold
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from sklearn.linear_model import LinearRegression, Ridge
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor

import joblib
import json

# ================== Load data ==================
file_path = "data/vietnam_housing_dataset.csv"
df = pd.read_csv(file_path)

# ================== TÃ¡ch tá»‰nh tá»« cá»™t Address ==================
df["Province"] = df["Address"].apply(lambda x: x.split(",")[-1].strip() if pd.notnull(x) else "KhÃ¡c")

# ================== Chuáº©n hÃ³a tÃªn tá»‰nh/thÃ nh ==================
province_mapping = {
    "HN": "HÃ  Ná»™i", "HaÌ€ NÃ´Ì£i": "HÃ  Ná»™i", "HÃ  Ná»™i.": "HÃ  Ná»™i",
    "TP Há»“ ChÃ­ Minh": "Há»“ ChÃ­ Minh", "TP. HCM": "Há»“ ChÃ­ Minh",
    "TPHCM": "Há»“ ChÃ­ Minh", "TpHCM": "Há»“ ChÃ­ Minh",
    "Há»“ ChÃ­ Minh.": "Há»“ ChÃ­ Minh", "Há»“ ChÃ­ MÃ­nh": "Há»“ ChÃ­ Minh",
    "Há»“ ChÃ­ Minh giÃ¡ 2tá»·380": "Há»“ ChÃ­ Minh", "Quáº­n 8": "Há»“ ChÃ­ Minh",
    "Quáº­n BÃ¬nh Tháº¡nh": "Há»“ ChÃ­ Minh", "TP. Cam Ranh": "KhÃ¡nh HÃ²a",
    "Quáº­n Nam Tá»« LiÃªm": "HÃ  Ná»™i",
    "BÃ  Rá»‹a VÅ©ng TÃ u.": "BÃ  Rá»‹a VÅ©ng TÃ u", "BÃ¬nh DÆ°Æ¡ng.": "BÃ¬nh DÆ°Æ¡ng",
    "BÃ¬nh DÆ°Æ¡ng (gáº§n cafe XÃ³m Váº¯ng 2)": "BÃ¬nh DÆ°Æ¡ng", "BÃ¬nh PhÆ°á»›c.": "BÃ¬nh PhÆ°á»›c",
    "BÃ¬nh Thuáº­n.": "BÃ¬nh Thuáº­n", "BÃ¬nh Äá»‹nh.": "BÃ¬nh Äá»‹nh",
    "Báº¡c LiÃªu.": "Báº¡c LiÃªu", "Báº¯c Giang.": "Báº¯c Giang",
    "Báº¯c Ninh.": "Báº¯c Ninh", "Báº¿n Tre.": "Báº¿n Tre", "Cáº§n ThÆ¡.": "Cáº§n ThÆ¡",
    "HÆ°ng YÃªn.": "HÆ°ng YÃªn", "Háº£i PhÃ²ng.": "Háº£i PhÃ²ng",
    "KhÃ¡nh HÃ²a.": "KhÃ¡nh HÃ²a", "KiÃªn Giang.": "KiÃªn Giang",
    "Kon Tum.": "Kon Tum", "Long An.": "Long An", "LÃ o Cai.": "LÃ o Cai",
    "LÃ¢m Äá»“ng.": "LÃ¢m Äá»“ng", "PhÃº Thá».": "PhÃº Thá»", "PhÃº YÃªn.": "PhÃº YÃªn",
    "Quáº£ng Ninh.": "Quáº£ng Ninh", "Quáº£ng Ninh (NgÃ£ 3 Ä‘Æ°á»ng HÃ²n Gai cÅ©)": "Quáº£ng Ninh",
    "Thanh HÃ³a.": "Thanh HÃ³a", "ThÃ¡i NguyÃªn.": "ThÃ¡i NguyÃªn",
    "Thá»«a ThiÃªn Huáº¿.": "Thá»«a ThiÃªn Huáº¿", "TrÃ  Vinh.": "TrÃ  Vinh",
    "ÄÃ  Náºµng.": "ÄÃ  Náºµng", "Äáº¯k Láº¯k.": "Äáº¯k Láº¯k", "Äá»“ng Nai.": "Äá»“ng Nai",
    "giÃ¡ 6ty": "KhÃ¡c", "": "KhÃ¡c"
}
df["Province"] = df["Province"].replace(province_mapping)

valid_provinces = [
    "An Giang","BÃ  Rá»‹a VÅ©ng TÃ u","BÃ¬nh DÆ°Æ¡ng","BÃ¬nh PhÆ°á»›c","BÃ¬nh Thuáº­n","BÃ¬nh Äá»‹nh",
    "Báº¡c LiÃªu","Báº¯c Giang","Báº¯c Ninh","Báº¿n Tre","CÃ  Mau","Cáº§n ThÆ¡","Gia Lai",
    "HÃ  Giang","HÃ  Nam","HÃ  Ná»™i","HÃ  TÄ©nh","HÃ²a BÃ¬nh","HÆ°ng YÃªn","Háº£i DÆ°Æ¡ng",
    "Háº£i PhÃ²ng","Háº­u Giang","Há»“ ChÃ­ Minh","KhÃ¡nh HÃ²a","KiÃªn Giang","Kon Tum",
    "Long An","LÃ o Cai","LÃ¢m Äá»“ng","Láº¡ng SÆ¡n","Nam Äá»‹nh","Nghá»‡ An","Ninh BÃ¬nh",
    "Ninh Thuáº­n","PhÃº Thá»","PhÃº YÃªn","Quáº£ng BÃ¬nh","Quáº£ng Nam","Quáº£ng NgÃ£i",
    "Quáº£ng Ninh","Quáº£ng Trá»‹","SÃ³c TrÄƒng","SÆ¡n La","Thanh HÃ³a","ThÃ¡i BÃ¬nh",
    "ThÃ¡i NguyÃªn","Thá»«a ThiÃªn Huáº¿","Tiá»n Giang","TrÃ  Vinh","TuyÃªn Quang",
    "TÃ¢y Ninh","VÄ©nh Long","VÄ©nh PhÃºc","YÃªn BÃ¡i","Äiá»‡n BiÃªn","ÄÃ  Náºµng",
    "Äáº¯k Láº¯k","Äá»“ng Nai","Äá»“ng ThÃ¡p","KhÃ¡c"
]
df["Province"] = df["Province"].apply(lambda x: x if x in valid_provinces else "KhÃ¡c")

# Xuáº¥t danh sÃ¡ch tá»‰nh Ä‘Ã£ chuáº©n hÃ³a
provinces = sorted(df["Province"].dropna().unique().tolist())
with open("provinces.json", "w", encoding="utf-8") as f:
    json.dump(provinces, f, ensure_ascii=False, indent=4)
print("âœ… ÄÃ£ chuáº©n hÃ³a vÃ  lÆ°u danh sÃ¡ch tá»‰nh vÃ o provinces.json")

# ================== Xá»­ lÃ½ dá»¯ liá»‡u thiáº¿u ==================
fill_with_mode = lambda x: x.fillna(x.mode()[0]) if not x.mode().empty else x
fill_with_median = lambda x: x.fillna(x.median())

df['Frontage'] = df.groupby('Address')['Frontage'].transform(fill_with_mode)
df['Access Road'] = df.groupby('Address')['Access Road'].transform(fill_with_mode)
df['Floors'] = df.groupby('Address')['Floors'].transform(fill_with_mode)

df['Floors'].fillna(df['Floors'].median(), inplace=True)
df['House direction'].fillna('N/A', inplace=True)
df['Balcony direction'].fillna('N/A', inplace=True)

df['Bedrooms'] = df.groupby('Address')['Bedrooms'].transform(fill_with_median)
df['Bathrooms'] = df.groupby('Address')['Bathrooms'].transform(fill_with_median)
df['Bedrooms'].fillna(df['Bedrooms'].median(), inplace=True)
df['Bathrooms'].fillna(df['Bathrooms'].median(), inplace=True)

df['Legal status'].fillna('Sale contract', inplace=True)
df['Furniture state'].fillna('N/A', inplace=True)

# ================== Xuáº¥t danh sÃ¡ch lá»±a chá»n cho dropdown ==================
house_directions = sorted(df['House direction'].dropna().unique().tolist())
balcony_directions = sorted(df['Balcony direction'].dropna().unique().tolist())
legal_statuses = sorted(df['Legal status'].dropna().unique().tolist())
furniture_states = sorted(df['Furniture state'].dropna().unique().tolist())

choices = {
    "house_directions": house_directions,
    "balcony_directions": balcony_directions,
    "legal_statuses": legal_statuses,
    "furniture_states": furniture_states
}
with open("choices.json", "w", encoding="utf-8") as f:
    json.dump(choices, f, ensure_ascii=False, indent=4)
print("âœ… ÄÃ£ lÆ°u choices.json cho dropdown")

# ================== Features & Target ==================
num_features = ['Area', 'Frontage', 'Access Road', 'Floors', 'Bedrooms', 'Bathrooms']
cat_features = ['House direction', 'Balcony direction', 'Legal status', 'Furniture state', 'Province']

X = df[num_features + cat_features]
y = df['Price']   # GiÃ¡ Ä‘Ã£ lÃ  Tá»¶ trong dataset gá»‘c

# ================== Preprocessor ==================
num_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
cat_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])
preprocessor = ColumnTransformer(
    transformers=[
        ('num', num_transformer, num_features),
        ('cat', cat_transformer, cat_features)
])

# ================== Train/Test Split ==================
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ================== Models ==================
models = {
    "linear_regression": LinearRegression(),
    "ridge_regression": Ridge(alpha=1.0),
    "random_forest": RandomForestRegressor(random_state=42, n_jobs=-1),
    "gradient_boosting": GradientBoostingRegressor(random_state=42),
}
try:
    from xgboost import XGBRegressor
    models["xgboost"] = XGBRegressor(random_state=42, n_jobs=-1)
except ImportError:
    print("âš ï¸ XGBoost chÆ°a cÃ i, bá» qua.")

# ================== Hyperparameter grids ==================
param_grids = {
    "random_forest": {
        "model__n_estimators": [50, 100, 150],
        "model__max_depth": [10, 20, None],
        "model__min_samples_split": [2, 5, 10],
    },
    "gradient_boosting": {
        "model__n_estimators": [100, 200, 300],
        "model__learning_rate": [0.05, 0.1, 0.2],
        "model__max_depth": [3, 5, 7],
    },
}

# ================== Train + Evaluate ==================
results = []
os.makedirs("models", exist_ok=True)
cv = KFold(n_splits=5, shuffle=True, random_state=42)

def evaluate_model(name, model, X_train, y_train, X_test, y_test):
    """Train, evaluate and save model + metrics"""
    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring="r2")
    cv_mean, cv_std = scores.mean(), scores.std()
    print(f"ğŸ“Š {name} CV RÂ² = {cv_mean:.3f} Â± {cv_std:.3f}")

    y_pred = model.predict(X_test)
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    r2 = r2_score(y_test, y_pred)
    print(f"ğŸ“Œ {name}: MAE={mae:.2f} tá»·, RMSE={rmse:.2f} tá»·, RÂ²={r2:.3f}")

    joblib.dump(model, f"models/{name}.joblib")

    return {
        "model": name,
        "CV_R2_mean": round(cv_mean, 3),
        "CV_R2_std": round(cv_std, 3),
        "MAE": round(mae, 2),
        "RMSE": round(rmse, 2),
        "R2": round(r2, 3)
    }

for name, model in models.items():
    print(f"\nğŸ”„ Training model: {name}")
    pipe = Pipeline(steps=[("preprocessor", preprocessor), ("model", model)])
    
    if name in param_grids:
        search = RandomizedSearchCV(
            pipe,
            param_distributions=param_grids[name],
            n_iter=10,
            scoring="r2",
            cv=cv,
            n_jobs=1,
            random_state=42
        )
        search.fit(X_train, y_train)
        best_model = search.best_estimator_
        print(f"ğŸ” {name} best params: {search.best_params_}")
    else:
        best_model = pipe.fit(X_train, y_train)
    
    metrics = evaluate_model(name, best_model, X_train, y_train, X_test, y_test)
    results.append(metrics)

results_df = pd.DataFrame(results)
results_df.to_csv("results.csv", index=False)
print("\nâœ… Training xong, models Ä‘Ã£ lÆ°u trong thÆ° má»¥c models/ vÃ  káº¿t quáº£ trong results.csv")
